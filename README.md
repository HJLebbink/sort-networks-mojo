# High Performance Sorting in Mojo

Efficient sorting in Modular Mojo optimized for small datasets (with a number of elements less than or equal to 64).

The primary objective is to create a drop-in replacement for the `sort[type: DType](inout v: DynamicVector[SIMD[type, 1]])`
function, utilizing sorting networks when the dataset is 64 elements or fewer. However, there are still a few areas that need refinement.

The sorting networks are shamelessly borrowed from the work of [Bert Dobbelaere](https://bertdobbelaere.github.io/sorting_networks_extended.html) who did all the hard searching!

## Performance compared to stdlib sort

I would love to present comprehensive scientific results, complete with boxplots, once a proper statistics library is available for computing standard deviations and confidence intervals. 
If you find yourself in need of ideas for a useful Mojo project, please consider it. In the meantime, humble time taken (in ms) of the minimum of 1_000_000 runs is what I can present.


In the `mojo` column, you'll find a call to: `sort[type: DType](inout v: DynamicVector[SIMD[type, 1]]` with the specified type and vector size. Under the `netw` column is a call to the sorting network: `fn sort_network[T: DType, width: Int, assending: Bool = True](v: SIMD[T, width]) -> SIMD[T, width]`. If you are sceptical (and you should be), please take a look at the code in `test_performance` function.


```
uint64  8       37      24
uint64  16      97      33
uint64  32      312     58

int64   8       38      24
int64   16      95      33
int64   32      305     58

float64 8       35      25
float64 16      98      33
float64 32      323     43

uint32  8       35      19
uint32  16      67      24
uint32  32      292     34

int32   8       34      19
int32   16      78      24
int32   32      270     34

float32 8       33      23
float32 16      79      32
float32 32      300     39

uint16  8       33      18
uint16  16      53      36
uint16  32      266     34

int16   8       34      18
int16   16      54      36
int16   32      243     34

float16 8       37      25
float16 16      51      38
float16 32      270     45

uint8   8       32      18
uint8   16      57      24
uint8   32  [CRASH]

int8    16  [CRASH]
int8    16      54      23
int8    16  [CRASH]
```

Overall, a sorting network is about 4 times faster.

Note that sorts of size 64 are currently not reported due to a bug. If you are in a position to address this issue, please take a look at https://github.com/modularml/mojo/issues/1505.

Note that the performance of float code is notably different compared to sorts with integer of the same size. I think it can be attributed to nan checking, as explained later on.

## How does it work?

A sorting network represents the smallest number of comparisons and swaps required to sort an array. For instance, the sorting network for 16 inputs has 61 compare/exchange elements (CEs) organized into 9 layers. Layers consist of parallel CE operations, allowing them to be executed in any order. However, the order of the layers remains fixed.

![net16](https://github.com/HJLebbink/sort-networks-mojo/blob/main/img/sort-network-16.png "Sorting Network 16")

The above sorting network has been proven to be minimal [https://arxiv.org/abs/1310.6271], no need to worry about that. What remains is our quest to find the most efficient method to implement this on our current hardware.

## Is the code efficient?
I like to restrict this question to code generated by the Mojo compiler (version 0.6.0) for AVX-512 capabable architectures. 

Next is the assembly code of one of the nine layers in a network that sorts 16 uint32 elements.

```asm
vmovdqa64  zmm0, ZMMWORD PTR [r13+rax*1+0x0]
vpermd     zmm3, zmm0, zmm1
vpminud    zmm2, zmm1, zmm3
mov        ax, 0xb552
kmovd      k1, eax
vpmaxud    zmm2{k1}, zmm1, zmm3
```

To start, `zmm0` is loaded with permutation indices, which hold the static information in the layer indicating how elements should be exchanged.
 In the subsequent `vpermd` instruction, the data in zmm1 is permuted and stored in `zmm3`.

We then obtain the minimum (`vpminud`) between the original data (`zmm1`) and the permuted data (`zmm0`), storing the result in `zmm2`. 
Here comes a clever trick â€“ we also compute the maximum values (`vpmaxud`), and only overwrite the minimum values based on a static 
mask (`k1`) that indicates the lower side of the compare/exchange element.

Repeat this for all layers and you sorted the data without any branches, and with minimal memory access. For sorting 16 uint32 values, I can't think of anything more efficient.

## Why Mojo?

I view Mojo as a smart assembler. While I would love to manually write all the sorting functionality in assembly, the myriad combinations of array 
lengths and data types make it somewhat impractical. Luckily, Mojo diligently generates similarly efficient code for int32, int16, sorting in ascending or descending order, and more.

Is the Mojo code flawless? No, you could blame LLVM for the following unnecessary nan check:

```asm
vmovaps     zmm0, ZMMWORD PTR [r15+rax*1]
vpermps     zmm0, zmm0, zmm1
vminps      zmm2, zmm0, zmm1
vcmpunordps k1, zmm1, zmm1
vmovaps     zmm2{k1}, zmm0
vmaxps      zmm1, zmm0, zmm1
vmovaps     zmm1{k1}, zmm0
mov         ax, 0xb552
kmovd       k1, eax
vmovaps     zmm2{k1}, zmm1
```

Compared to the code for sorting 16 uint16 values, the first three instructions are unchanged (but are now for float32 instead of uint32). 
The [`vcmpunordps`](https://github.com/HJLebbink/asm-dude/wiki/CMPPS) instruction is new, which stores in mask `k1` the values in the data 
(`zmm1`) that are nan. However, there are several reasons why there cannot be any nans in `zmm1`. The simplest reason is that the previous layer
 already includes the exact same nan tests.

Next, the minimum and maximum values, which happen to contain no nan values, are overwritten with the permuted data 
(which could also contain nan values, but that doesn't seem to be of interest). Removing the nan tests would result 
in the same optimal code. If there were a way to toy with the strictness of floating points, perhaps this unnecessary code could be trimmed. If you know a way, let me know!